{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaH6-z_OKyRj",
    "outputId": "45e117ea-d35e-4899-ae0c-ccd4f7cef605"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import copy\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torchvision.models._api import WeightsEnum\n",
    "from torch.hub import load_state_dict_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_setup import copy_images_to_folders, create_folders, remove_folders, split_data_for_class, split_data\n",
    "from image_check import imshow\n",
    "from model import CustomEfficientNetB0, CustomLoss, compute_class_weights, normalize_matrix, print_matrix, validate_model, train_model, plot_metrics, visualize_model\n",
    "from uncertainty_metrics import calculate_risks, process_labels, calculate_and_append_risks, calculate_and_append_risks_by_class, calculate_softmax_uncertainties, calculate_top2_softmax_uncertainties, calculate_random_uncertainties, calculate_mc_dropout_uncertainties_by_sample, calculate_mc_dropout_uncertainties_by_class, calculate_variance_uncertainties, calculate_variational_ratio_uncertainties, calculate_entropy_uncertainties, calculate_predictive_entropy_uncertainties, calculate_mutual_information_uncertainties, smooth_calcs, calculate_aurc, plot_risk_coverage, process_uncertainties, calculate_variational_ratio_dropout_uncertainties, calculate_mutual_information_mc_dropout, select_desired_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best algorithm for hardware\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# interactive mode for graph plot\n",
    "plt.ion()\n",
    "\n",
    "# set device to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CSV_PATH = 'data/ISIC_2019_Training_GroundTruth.csv'\n",
    "IMAGE_FOLDER = 'data/ISIC_2019_Resized'\n",
    "SORTED_FOLDER = 'data/ISIC_Sorted'\n",
    "SUBSET_FOLDER = 'data/data_subset'\n",
    "SAMPLE_LIMIT = None  # None or a number up to 12875\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "SET_BATCH_SIZE = 64\n",
    "\n",
    "#sample numbers by class copied here for reference\n",
    "#MEL\tNV\t    BCC\t    AK\t BKL\tDF\tVASC  SCC\tUNK\n",
    "#4522\t12875\t3323\t867\t 2624\t239\t253\t  628\t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation for ratios\n",
    "if TRAIN_RATIO + VAL_RATIO + TEST_RATIO != 1:\n",
    "    print(\"Invalid data ratios\")\n",
    "else:\n",
    "    print(\"Data ratios are valid\")\n",
    "\n",
    "    # Perform operations\n",
    "    copy_images_to_folders(CSV_PATH, IMAGE_FOLDER, SORTED_FOLDER)\n",
    "    print(\"images split into classes\")\n",
    "    \n",
    "    remove_folders(os.path.join(SUBSET_FOLDER, 'train'), os.path.join(SUBSET_FOLDER, 'val'), os.path.join(SUBSET_FOLDER, 'test'))\n",
    "    \n",
    "    split_data(SORTED_FOLDER, os.path.join(SUBSET_FOLDER, 'train'), os.path.join(SUBSET_FOLDER, 'val'), os.path.join(SUBSET_FOLDER, 'test'), train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO, test_ratio=TEST_RATIO, sample_limit=SAMPLE_LIMIT, seed=6861611)\n",
    "    print(\"data split into train, validation and test sets for each class\")\n",
    "    \n",
    "    # Remove unknown folder as there are no samples\n",
    "    remove_folders(os.path.join(SUBSET_FOLDER, 'train', 'UNK'), os.path.join(SUBSET_FOLDER, 'val', 'UNK'), os.path.join(SUBSET_FOLDER, 'test', 'UNK'))\n",
    "    print(\"UNK folders removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XVgfadKDdXE"
   },
   "outputs": [],
   "source": [
    "#transform data\n",
    "data_transforms = {\n",
    "'train': transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=360),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), shear=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "'val': transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "'test': transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets and dataloaders\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(SUBSET_FOLDER, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=SET_BATCH_SIZE, shuffle=True, num_workers=2) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Adjusted way to obtain class names\n",
    "class_names = image_datasets['train'].classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"file: \", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "ZGxDjkKlN9Z-",
    "outputId": "1acf728e-a451-4471-ba8d-f956c03624a5"
   },
   "outputs": [],
   "source": [
    "# Specify the number of images to display\n",
    "num_images = 4\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Select a subset of images\n",
    "inputs_subset = inputs[:num_images]\n",
    "classes_subset = classes[:num_images]\n",
    "\n",
    "# Make a grid from the subset\n",
    "out = torchvision.utils.make_grid(inputs_subset)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict(self, *args, **kwargs):\n",
    "    kwargs.pop(\"check_hash\")\n",
    "    return load_state_dict_from_url(self.url, *args, **kwargs)\n",
    "WeightsEnum.get_state_dict = get_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert targets to tensor and move to device\n",
    "targets_tensor = torch.tensor(image_datasets['train'].targets).to(device)\n",
    "\n",
    "# Compute class weights\n",
    "no_weights = None\n",
    "custom_weights = torch.FloatTensor([82, 112, 52, 52, 481, 52, 481, 82]).to(device)\n",
    "equal_weights = torch.FloatTensor(compute_class_weights(targets_tensor, len(image_datasets['train'].classes))).to(device)\n",
    "equal_custom_weights = (equal_weights * custom_weights).to(device)\n",
    "\n",
    "# Print class weights\n",
    "print(\"Base Weights:\", no_weights)\n",
    "print(\"Custom Weights:\", custom_weights)\n",
    "print(\"Equal Weights:\", equal_weights)\n",
    "print(\"Equal Custom Weights:\", equal_custom_weights)\n",
    "print()\n",
    "\n",
    "weights = [no_weights, custom_weights, equal_weights, equal_custom_weights]\n",
    "\n",
    "save_dirs = [\"no_class_weights\", \"custom_weights\", \"equal_weights\", \"equal_custom_weights\"]\n",
    "models = []\n",
    "criterions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom model with dropout\n",
    "model_ft = CustomEfficientNetB0(num_classes=num_classes, dropout_prob=0.4).to(device)\n",
    "\n",
    "# Use the Adam optimizer for training\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# Define a learning rate scheduler to decay the learning rate\n",
    "# by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "for index, class_weights in enumerate(weights):\n",
    "    print(f\"Weights: {save_dirs[index]}\\n\")\n",
    "\n",
    "    models.append(model_ft)\n",
    "    \n",
    "    # Define the loss function (cross-entropy loss)\n",
    "    criterion = nn.CrossEntropyLoss(weight=(class_weights), reduction='mean').to(device)\n",
    "\n",
    "    criterions.append(criterion)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_ft in enumerate(models):\n",
    "    print(f\"Weights: {save_dirs[index]}\\n\")\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_epoch = 1\n",
    "    \n",
    "    train_model(model_ft, criterions[index], optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes, device, train_losses, train_accuracies, val_losses, val_accuracies, best_epoch, num_epochs=10, num_val_mc_samples=100, loss_weight=1, acc_weight=0, num_classes=num_classes, save_dir=save_dirs[index], resume_training=False)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_ft in enumerate(models):\n",
    "    print(f\"Weights: {save_dirs[index]}\\n\")\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_epoch = 1\n",
    "    \n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(os.path.join(save_dirs[index], 'checkpoint.pth.tar'))\n",
    "    \n",
    "    # Load model state_dict\n",
    "    model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load optimizer state_dict\n",
    "    optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Load scheduler state_dict\n",
    "    exp_lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Retrieve other variables\n",
    "    best_combined_metric = checkpoint['best_combined_metric']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    best_val_acc = checkpoint['best_val_acc']\n",
    "    best_epoch = checkpoint['best_epoch']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    train_accuracies = checkpoint['train_accuracies']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_accuracies = checkpoint['val_accuracies']\n",
    "    \n",
    "    plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, best_epoch)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_ft in enumerate(models):\n",
    "    print(f\"Weights: {save_dirs[index]}\\n\")\n",
    "    \n",
    "    visualize_model(model_ft, dataloaders['val'], device, class_names, num_images=4)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_risks_list = []\n",
    "master_labels_list = []\n",
    "\n",
    "master_risks_list_by_class = []\n",
    "master_labels_list_by_class = []\n",
    "\n",
    "for index, model_ft in enumerate(models):\n",
    "    print(f\"Weights: {save_dirs[index]}\\n\")\n",
    "    \n",
    "    # Lists to store results\n",
    "    risks_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    risks_list_by_class = [[] for _ in range(len(class_names))]\n",
    "    labels_list_by_class = [[] for _ in range(len(class_names))]\n",
    "    \n",
    "    uncertainty_functions = [\n",
    "        (calculate_softmax_uncertainties, \"Softmax Response\"),\n",
    "        (calculate_top2_softmax_uncertainties, \"Top2 Softmax Difference\"),\n",
    "        (calculate_random_uncertainties, \"Random Uncertainties\"),\n",
    "        (calculate_mc_dropout_uncertainties_by_sample, \"MCD By Sample\", {\"num_samples\": 100}),\n",
    "        (calculate_mc_dropout_uncertainties_by_class, \"MCD By Class\", {\"num_samples\": 100}),\n",
    "        (calculate_variance_uncertainties, \"Variance\"),\n",
    "        (calculate_variational_ratio_uncertainties, \"Variational Ratio\"),\n",
    "        (calculate_variational_ratio_dropout_uncertainties, \"Variational Ratio with Dropout\", {\"num_samples\": 100}),\n",
    "        (calculate_entropy_uncertainties, \"Entropy\"),\n",
    "        (calculate_predictive_entropy_uncertainties, \"Predictive Entropy\", {\"num_samples\": 100}),\n",
    "        (calculate_mutual_information_uncertainties, \"Mutual Information\"),\n",
    "        (calculate_mutual_information_mc_dropout, \"Mutual Information with Dropout\", {\"num_samples\": 100})\n",
    "    ]\n",
    "    \n",
    "    for function, name, *args in uncertainty_functions:\n",
    "        if args:  # Check if additional arguments exist\n",
    "            additional_args = args[0]  # Extract additional arguments\n",
    "            process_uncertainties(risks_list, labels_list, risks_list_by_class, labels_list_by_class, model_ft, dataloaders['test'], class_names, device, function, name, **additional_args)  # Pass additional arguments as keyword arguments\n",
    "        else:\n",
    "            process_uncertainties(risks_list, labels_list, risks_list_by_class, labels_list_by_class, model_ft, dataloaders['test'], class_names, device, function, name)\n",
    "\n",
    "    master_risks_list.append(risks_list)\n",
    "    master_labels_list.append(labels_list)\n",
    "    \n",
    "    master_risks_list_by_class.append(risks_list_by_class)\n",
    "    master_labels_list_by_class.append(labels_list_by_class)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired metrics to plot\n",
    "# comment out undesired metrics\n",
    "desired_metrics = [\n",
    "    \"Softmax Response\",\n",
    "    \"Top2 Softmax Difference\",\n",
    "    \"Random Uncertainties\",\n",
    "    \"MCD By Sample\",\n",
    "    \"MCD By Class\",\n",
    "    \"Variance\",\n",
    "    \"Variational Ratio\",\n",
    "    \"Variational Ratio with Dropout\",\n",
    "    \"Entropy\",\n",
    "    \"Predictive Entropy\",\n",
    "    \"Mutual Information\",\n",
    "    \"Mutual Information with Dropout\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_ft in enumerate(models):\n",
    "    print(f\"Weights: {save_dirs[index]}\\n\")\n",
    "    # Select desired metrics for all samples\n",
    "    selected_labels_list, selected_risks_list = select_desired_metrics(master_labels_list[index], master_risks_list[index], desired_metrics)\n",
    "    \n",
    "    # Print information about all classes\n",
    "    print(f\"All Classes: {len(selected_risks_list[0])} samples\")\n",
    "    x_smooth_percentage_interp, x_smooth_percentage = smooth_calcs(selected_risks_list[0])\n",
    "    calculate_aurc(selected_risks_list, selected_labels_list, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "    plot_risk_coverage(selected_risks_list, selected_labels_list, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_ft in enumerate(models):\n",
    "    # Iterate over each class\n",
    "    for i in range(len(class_names)):\n",
    "        # Select desired metrics for the current class\n",
    "        selected_labels_list_class, selected_risks_list_class = select_desired_metrics(master_labels_list_by_class[index][i], master_risks_list_by_class[index][i], desired_metrics)\n",
    "        \n",
    "        # Print information about the current class\n",
    "        print(f\"{class_names[i]}: {len(selected_risks_list_class[0])} samples\")\n",
    "        x_smooth_percentage_interp, x_smooth_percentage = smooth_calcs(selected_risks_list_class[0])\n",
    "        calculate_aurc(selected_risks_list_class, selected_labels_list_class, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "        plot_risk_coverage(selected_risks_list_class, selected_labels_list_class, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD STUFF BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65NHU01pOODQ",
    "outputId": "ce37c308-f9cc-49f5-cca4-85ca4cc0ca6a"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_epoch = 1\n",
    "\n",
    "# Initialize the custom model with dropout\n",
    "model_ft = CustomEfficientNetB0(num_classes=num_classes, dropout_prob=0.4)\n",
    "\n",
    "# Move the model to the specified device (e.g., GPU or CPU)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Define the loss function (cross-entropy loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use the Adam optimizer for training\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# Define a learning rate scheduler to decay the learning rate\n",
    "# by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert targets to tensor and move to device\n",
    "targets_tensor = torch.tensor(image_datasets['train'].targets).to(device)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weights(targets_tensor, len(image_datasets['train'].classes))\n",
    "\n",
    "# Print class weights\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class weights matrices and move to device\n",
    "equal_class_weights_matrix = torch.tensor([\n",
    "    [0, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 0, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 0, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 0]\n",
    "], dtype=torch.float).to(device)\n",
    "\n",
    "custom_class_weights_matrix = torch.tensor([\n",
    "    [0,  1,  20,  20,  10, 20,  10, 1],\n",
    "    [1,  0,  30,  30,  10, 30,  10, 1],\n",
    "    [10, 10, 0,   1,   10, 1,   10, 10],\n",
    "    [10, 10, 1,   0,   10, 1,   10, 10],\n",
    "    [10, 10, 150, 150, 0,  150, 1,  10],\n",
    "    [10, 10, 1,   1,   10, 0,   10, 10],\n",
    "    [10, 10, 150, 150, 1,  150, 0,  10],\n",
    "    [1,  1,  20,  20,  10, 20,  10, 0]\n",
    "], dtype=torch.float).to(device)\n",
    "\n",
    "equal_class_weights_matrix = normalize_matrix(equal_class_weights_matrix)\n",
    "custom_class_weights_matrix = normalize_matrix(custom_class_weights_matrix)\n",
    "\n",
    "# Print the normalized matrices\n",
    "print_matrix(\"Equal Cost Matrix\", equal_class_weights_matrix)\n",
    "print_matrix(\"Custom Cost Matrix\", custom_class_weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the custom class weights matrix using the calculated class weights\n",
    "balanced_class_weights_matrix = custom_class_weights_matrix * torch.tensor(class_weights, dtype=torch.float).unsqueeze(0).to(device) * 7\n",
    "\n",
    "print_matrix(\"Balanced Class Weights Matrix\", balanced_class_weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function using the custom loss\n",
    "criterion = CustomLoss(balanced_class_weights_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes, device, train_losses, train_accuracies, val_losses, val_accuracies, best_epoch, num_epochs=25, num_val_mc_samples=100, loss_weight=1, acc_weight=0, num_classes=num_classes, save_dir=\"custom_weights_matrix\", resume_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('test/checkpoint.pth.tar')\n",
    "\n",
    "# Load model state_dict\n",
    "model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load optimizer state_dict\n",
    "optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Load scheduler state_dict\n",
    "exp_lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "# Retrieve other variables\n",
    "best_combined_metric = checkpoint['best_combined_metric']\n",
    "best_val_loss = checkpoint['best_val_loss']\n",
    "best_val_acc = checkpoint['best_val_acc']\n",
    "best_epoch = checkpoint['best_epoch']\n",
    "train_losses = checkpoint['train_losses']\n",
    "train_accuracies = checkpoint['train_accuracies']\n",
    "val_losses = checkpoint['val_losses']\n",
    "val_accuracies = checkpoint['val_accuracies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_ft, dataloaders['val'], device, class_names, num_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store results\n",
    "risks_list = []\n",
    "labels_list = []\n",
    "\n",
    "risks_list_by_class = [[] for _ in range(len(class_names))]\n",
    "labels_list_by_class = [[] for _ in range(len(class_names))]\n",
    "\n",
    "uncertainty_functions = [\n",
    "    (calculate_softmax_uncertainties, \"Softmax Response\"),\n",
    "    (calculate_top2_softmax_uncertainties, \"Top2 Softmax Difference\"),\n",
    "    (calculate_random_uncertainties, \"Random Uncertainties\"),\n",
    "    (calculate_mc_dropout_uncertainties_by_sample, \"MCD By Sample\", {\"num_samples\": 1}),\n",
    "    (calculate_mc_dropout_uncertainties_by_class, \"MCD By Class\", {\"num_samples\": 1}),\n",
    "    (calculate_variance_uncertainties, \"Variance\"),\n",
    "    (calculate_variational_ratio_uncertainties, \"Variational Ratio\"),\n",
    "    (calculate_variational_ratio_dropout_uncertainties, \"Variational Ratio with Dropout\", {\"num_samples\": 1}),\n",
    "    (calculate_entropy_uncertainties, \"Entropy\"),\n",
    "    (calculate_predictive_entropy_uncertainties, \"Predictive Entropy\", {\"num_samples\": 1}),\n",
    "    (calculate_mutual_information_uncertainties, \"Mutual Information\"),\n",
    "    (calculate_mutual_information_mc_dropout, \"Mutual Information with Dropout\", {\"num_samples\": 1})\n",
    "]\n",
    "\n",
    "for function, name, *args in uncertainty_functions:\n",
    "    if args:  # Check if additional arguments exist\n",
    "        additional_args = args[0]  # Extract additional arguments\n",
    "        process_uncertainties(risks_list, labels_list, risks_list_by_class, labels_list_by_class, model_ft, dataloaders['test'], class_names, device, function, name, **additional_args)  # Pass additional arguments as keyword arguments\n",
    "    else:\n",
    "        process_uncertainties(risks_list, labels_list, risks_list_by_class, labels_list_by_class, model_ft, dataloaders['test'], class_names, device, function, name)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_list = [\n",
    "    \"Softmax Response measures the model's confidence in its predictions based on the softmax probabilities.\",\n",
    "    \"Top2 Softmax Difference measures the uncertainty by calculating the difference between the top two softmax probabilities\",\n",
    "    \"Random Uncertainties assigns random uncertainty values to each prediction, providing a baseline comparison for uncertainty estimation methods.\",\n",
    "    \"MCD By Sample utilizes Monte Carlo Dropout (MCD) to estimate uncertainties by averaging predictions across multiple samples with dropout.\",\n",
    "    \"MCD By Class employs Monte Carlo Dropout (MCD) to estimate uncertainties by averaging predictions across multiple samples with dropout for each class.\",\n",
    "    \"Variance calculates uncertainty by computing the variance of softmax probabilities across classes for each prediction.\",\n",
    "    \"Variational Ratio calculates uncertainty by computing the ratio of the maximum softmax probability (mode probability) to the maximum probability among other classes for each prediction.\",\n",
    "    \"Entropy calculates uncertainty by measuring the entropy of softmax probabilities for each prediction.\",\n",
    "    \"Predictive Entropy estimates uncertainty by averaging the entropy of softmax probabilities across multiple samples generated with dropout.\",\n",
    "    \"Mutual Information computes uncertainty by measuring the mutual information between the model's softmax probabilities and a uniform distribution.\"\n",
    "]\n",
    "\n",
    "for description in description_list:\n",
    "    print(f\"- {description}\")\n",
    "\n",
    "print(\n",
    "\"\"\"\n",
    "- Risk = 1 - Accuracy, where Accuracy is the mean accuracy (normalised) over the samples included in the Coverage.\n",
    "- Coverage is the normalised number of samples over the total samples. Coverage decreases as the most uncertain samples are removed.\n",
    "- Area Under Risk Coverage (AURC) is the area under the Risk Coverage curve.\n",
    "\n",
    "- Classes:\n",
    "    - Melanoma (MEL)\n",
    "    - Melanocytic nevus (NV)\n",
    "    - Basal cell carcinoma (BCC)\n",
    "    - Actinic keratosis (AK)\n",
    "    - Benign keratosis (BKL) \n",
    "        [solar lentigo / seborrheic keratosis / lichen planus-like keratosis]\n",
    "    - Dermatofibroma (DF)\n",
    "    - Vascular lesion (VASC)\n",
    "    - Squamous cell carcinoma (SCC)\n",
    "    - None of the others (UNK)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg6WwxuGrOsn"
   },
   "outputs": [],
   "source": [
    "# Define the desired metrics to plot\n",
    "# comment out undesired metrics\n",
    "desired_metrics = [\n",
    "    \"Softmax Response\",\n",
    "    \"Top2 Softmax Difference\",\n",
    "    \"Random Uncertainties\",\n",
    "    \"MCD By Sample\",\n",
    "    \"MCD By Class\",\n",
    "    \"Variance\",\n",
    "    \"Variational Ratio\",\n",
    "    \"Variational Ratio with Dropout\",\n",
    "    \"Entropy\",\n",
    "    \"Predictive Entropy\",\n",
    "    \"Mutual Information\",\n",
    "    \"Mutual Information with Dropout\"\n",
    "]\n",
    "\n",
    "# Select desired metrics for all samples\n",
    "selected_labels_list, selected_risks_list = select_desired_metrics(labels_list, risks_list, desired_metrics)\n",
    "\n",
    "# Print information about all classes\n",
    "print(f\"All Classes: {len(selected_risks_list[0])} samples\")\n",
    "x_smooth_percentage_interp, x_smooth_percentage = smooth_calcs(selected_risks_list[0])\n",
    "calculate_aurc(selected_risks_list, selected_labels_list, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "plot_risk_coverage(selected_risks_list, selected_labels_list, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "\n",
    "# Iterate over each class\n",
    "for i in range(len(class_names)):\n",
    "    # Select desired metrics for the current class\n",
    "    selected_labels_list_class, selected_risks_list_class = select_desired_metrics(labels_list_by_class[i], risks_list_by_class[i], desired_metrics)\n",
    "    \n",
    "    # Print information about the current class\n",
    "    print(f\"{class_names[i]}: {len(selected_risks_list_class[0])} samples\")\n",
    "    x_smooth_percentage_interp, x_smooth_percentage = smooth_calcs(selected_risks_list_class[0])\n",
    "    calculate_aurc(selected_risks_list_class, selected_labels_list_class, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "    plot_risk_coverage(selected_risks_list_class, selected_labels_list_class, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "    print()\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
