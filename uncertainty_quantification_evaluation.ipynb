{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wglE1uRQYql"
   },
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaH6-z_OKyRj",
    "outputId": "45e117ea-d35e-4899-ae0c-ccd4f7cef605"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import copy\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torchvision.models._api import WeightsEnum\n",
    "from torch.hub import load_state_dict_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_setup import copy_images_to_folders\n",
    "from image_check import imshow\n",
    "from model import CustomEfficientNetB0, validate_model, train_model, plot_metrics, visualize_model\n",
    "from uncertainty_metrics import calculate_risks, process_labels, calculate_and_append_risks, calculate_and_append_risks_by_class, calculate_softmax_uncertainties, calculate_top2_softmax_uncertainties, calculate_random_uncertainties, calculate_mc_dropout_uncertainties_by_sample, calculate_mc_dropout_uncertainties_by_class, calculate_variance_uncertainties, calculate_variational_ratio_uncertainties, calculate_entropy_uncertainties, calculate_predictive_entropy_uncertainties, calculate_mutual_information_uncertainties, smooth_calcs, calculate_aurc, plot_risk_coverage, process_uncertainties, calculate_variational_ratio_dropout_uncertainties, calculate_mutual_information_mc_dropout, select_desired_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best algorithm for hardware\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# interactive mode for graph plot\n",
    "plt.ion()\n",
    "\n",
    "# set device to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JU3HfHzSspI"
   },
   "source": [
    "Sort data into folders based on class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7txNfNASrDG"
   },
   "outputs": [],
   "source": [
    "csv_path = 'data/ISIC_2019_Training_GroundTruth.csv'\n",
    "image_folder = 'data/ISIC_2019_Resized'\n",
    "output_folder = 'data/ISIC_Sorted'\n",
    "\n",
    "copy_images_to_folders(csv_path, image_folder, output_folder)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSZnksOtT5GQ"
   },
   "source": [
    "Split data into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "ixMk4IkpER3K",
    "outputId": "8c2c4eac-efc0-473f-af44-e5caccf646c8"
   },
   "outputs": [],
   "source": [
    "# declare variables for model creation\n",
    "input_folder = 'data/ISIC_Sorted'  # This is the folder containing class subdirectories\n",
    "output_folder = 'data/data_subset'  # The desired output folder for \"training\" and \"validation\" folders\n",
    "sample_limit=None #None or a number up to 12875\n",
    "train_ratio=0.6\n",
    "val_ratio=0.2\n",
    "test_ratio=0.2\n",
    "\n",
    "if train_ratio+val_ratio+test_ratio != 1:\n",
    "    print(\"invalid data ratios\")\n",
    "\n",
    "set_batch_size=64\n",
    "\n",
    "#sample numbers by class copied here for reference\n",
    "#MEL\tNV\t    BCC\t    AK\t BKL\tDF\tVASC  SCC\tUNK\n",
    "#4522\t12875\t3323\t867\t 2624\t239\t253\t  628\t0\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = os.path.join(output_folder, 'train')\n",
    "validation_folder = os.path.join(output_folder, 'val')\n",
    "test_folder = os.path.join(output_folder, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D097WZYyDNLX"
   },
   "outputs": [],
   "source": [
    "# Remove existing \"training\" and \"validation\" folders if they exist\n",
    "shutil.rmtree(training_folder, ignore_errors=True)\n",
    "shutil.rmtree(validation_folder, ignore_errors=True)\n",
    "shutil.rmtree(test_folder, ignore_errors=True)\n",
    "\n",
    "# Create the \"training\" and \"validation\" folders\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "os.makedirs(validation_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# generate list of class names from the folder structure\n",
    "class_names = [class_name for class_name in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, class_name))]\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each class folder\n",
    "for class_name in os.listdir(input_folder):\n",
    "    class_folder_input = os.path.join(input_folder, class_name)\n",
    "    if os.path.isdir(class_folder_input):\n",
    "        # Get a list of all images in the class folder\n",
    "        images = [img for img in os.listdir(class_folder_input) if img.endswith('.jpg')]\n",
    "        \n",
    "        # Limit the number of samples if sample_limit is provided\n",
    "        if sample_limit is not None:\n",
    "            images = images[:sample_limit]\n",
    "\n",
    "        # Randomly shuffle the list of images\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(images)\n",
    "\n",
    "        # Calculate the number of samples for training and validation\n",
    "        class_size = len(images)\n",
    "        num_training_samples = int(class_size * train_ratio)\n",
    "        num_val_samples = int(class_size * val_ratio)\n",
    "        num_test_samples = int(class_size * test_ratio)\n",
    "\n",
    "        # Create the class subfolders within the output folders\n",
    "        class_folder_training = os.path.join(training_folder, class_name)\n",
    "        class_folder_validation = os.path.join(validation_folder, class_name)\n",
    "        class_folder_test = os.path.join(test_folder, class_name)\n",
    "        os.makedirs(class_folder_training, exist_ok=True)\n",
    "        os.makedirs(class_folder_validation, exist_ok=True)\n",
    "        os.makedirs(class_folder_test, exist_ok=True)\n",
    "\n",
    "        # Copy images to the training folder\n",
    "        for image_name in images[:num_training_samples]:\n",
    "            image_path = os.path.join(class_folder_input, image_name)\n",
    "            shutil.copy(image_path, class_folder_training)\n",
    "        # Copy images to the validation folder\n",
    "        for image_name in images[num_training_samples:num_training_samples + num_val_samples]:\n",
    "            image_path = os.path.join(class_folder_input, image_name)\n",
    "            shutil.copy(image_path, class_folder_validation)\n",
    "        # Copy images to the test folder\n",
    "        for image_name in images[num_training_samples + num_val_samples:]:\n",
    "            image_path = os.path.join(class_folder_input, image_name)\n",
    "            shutil.copy(image_path, class_folder_test)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miNkN44TDZFM"
   },
   "outputs": [],
   "source": [
    "#remove unknown folder as no samples\n",
    "shutil.rmtree(\"data/data_subset/train/UNK\", ignore_errors=True)\n",
    "shutil.rmtree(\"data/data_subset/val/UNK\", ignore_errors=True)\n",
    "shutil.rmtree(\"data/data_subset/test/UNK\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XVgfadKDdXE"
   },
   "outputs": [],
   "source": [
    "#transform data\n",
    "data_transforms = {\n",
    "'train': transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=360),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), shear=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "'val': transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "'test': transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4X4h6BniL9Vp",
    "outputId": "9954bcb6-c1cc-41f7-b6d8-8cf67d96198c"
   },
   "outputs": [],
   "source": [
    "# create datasets and dataloaders\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(output_folder, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=set_batch_size, shuffle=True, num_workers=2) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "#print checks to see if the values are the same - if yes then can remove the assignment here\n",
    "class_names = image_datasets['train'].classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"file: \", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFMOhsIdQov5"
   },
   "source": [
    "show sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "ZGxDjkKlN9Z-",
    "outputId": "1acf728e-a451-4471-ba8d-f956c03624a5"
   },
   "outputs": [],
   "source": [
    "# Specify the number of images to display\n",
    "num_images = 4\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Select a subset of images\n",
    "inputs_subset = inputs[:num_images]\n",
    "classes_subset = classes[:num_images]\n",
    "\n",
    "# Make a grid from the subset\n",
    "out = torchvision.utils.make_grid(inputs_subset)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes_subset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToECJPIxgkz0"
   },
   "source": [
    "temporary fix for efficientnet weights issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict(self, *args, **kwargs):\n",
    "    kwargs.pop(\"check_hash\")\n",
    "    return load_state_dict_from_url(self.url, *args, **kwargs)\n",
    "WeightsEnum.get_state_dict = get_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA6NdpeVQ0IU"
   },
   "source": [
    "model fitting configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65NHU01pOODQ",
    "outputId": "ce37c308-f9cc-49f5-cca4-85ca4cc0ca6a"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_epoch = 1\n",
    "\n",
    "# Initialize the custom model with dropout\n",
    "model_ft = CustomEfficientNetB0(num_classes=num_classes, dropout_prob=0.4)\n",
    "\n",
    "# Move the model to the specified device (e.g., GPU or CPU)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Define the loss function (cross-entropy loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use the Adam optimizer for training\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# Define a learning rate scheduler to decay the learning rate\n",
    "# by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_weights = {}\n",
    "total_samples = 0\n",
    "\n",
    "# Initialize class counts dictionary\n",
    "class_counts = {class_idx: 0 for class_idx in range(len(image_datasets['train'].classes))}\n",
    "\n",
    "# Iterate over training dataloader to count samples for each class\n",
    "for inputs, labels in dataloaders['train']:\n",
    "    for label in labels:\n",
    "        class_counts[label.item()] += 1\n",
    "        total_samples += 1\n",
    "\n",
    "weights_list = [82, 112, 52, 52, 481, 52, 481, 82]\n",
    "\n",
    "# Calculate class weights using the formula\n",
    "for class_idx, count in class_counts.items():\n",
    "    class_weights[class_idx] = (1 / count) * (total_samples / len(class_counts)) * weights_list[class_idx]\n",
    "\n",
    "# Print class weights\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Define the loss function (cross-entropy loss) with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('saved_models/checkpoint.pth.tar')\n",
    "\n",
    "# Load model state_dict\n",
    "model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load optimizer state_dict\n",
    "optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Load scheduler state_dict\n",
    "exp_lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "# Retrieve other variables\n",
    "best_combined_metric = checkpoint['best_combined_metric']\n",
    "best_val_loss = checkpoint['best_val_loss']\n",
    "best_val_acc = checkpoint['best_val_acc']\n",
    "best_epoch = checkpoint['best_epoch']\n",
    "train_losses = checkpoint['train_losses']\n",
    "train_accuracies = checkpoint['train_accuracies']\n",
    "val_losses = checkpoint['val_losses']\n",
    "val_accuracies = checkpoint['val_accuracies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes, device, train_losses, train_accuracies, val_losses, val_accuracies, best_epoch, num_epochs=2, num_val_mc_samples=2, loss_weight=1, acc_weight=0, num_classes=num_classes, save_dir=\"test_model\", resume_training=False)\n",
    "#train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes, device, train_losses, train_accuracies, val_losses, val_accuracies, best_epoch, num_epochs=5, num_val_mc_samples=100, loss_weight=1, acc_weight=0, num_classes=num_classes, save_dir=\"custom_weights_mult\", resume_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GboEK9R6Q6CQ"
   },
   "source": [
    "visualise trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_ft, dataloaders['val'], device, class_names, num_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store results\n",
    "risks_list = []\n",
    "labels_list = []\n",
    "\n",
    "risks_list_by_class = [[] for _ in range(len(class_names))]\n",
    "labels_list_by_class = [[] for _ in range(len(class_names))]\n",
    "\n",
    "uncertainty_functions = [\n",
    "    (calculate_softmax_uncertainties, \"Softmax Response\"),\n",
    "    (calculate_top2_softmax_uncertainties, \"Top2 Softmax Difference\"),\n",
    "    (calculate_random_uncertainties, \"Random Uncertainties\"),\n",
    "    (calculate_mc_dropout_uncertainties_by_sample, \"MCD By Sample\", {\"num_samples\": 100}),\n",
    "    (calculate_mc_dropout_uncertainties_by_class, \"MCD By Class\", {\"num_samples\": 100}),\n",
    "    (calculate_variance_uncertainties, \"Variance\"),\n",
    "    (calculate_variational_ratio_uncertainties, \"Variational Ratio\"),\n",
    "    (calculate_variational_ratio_dropout_uncertainties, \"Variational Ratio with Dropout\", {\"num_samples\": 100}),\n",
    "    (calculate_entropy_uncertainties, \"Entropy\"),\n",
    "    (calculate_predictive_entropy_uncertainties, \"Predictive Entropy\", {\"num_samples\": 100}),\n",
    "    (calculate_mutual_information_uncertainties, \"Mutual Information\"),\n",
    "    (calculate_mutual_information_mc_dropout, \"Mutual Information with Dropout\", {\"num_samples\": 100})\n",
    "]\n",
    "\n",
    "for function, name, *args in uncertainty_functions:\n",
    "    if args:  # Check if additional arguments exist\n",
    "        additional_args = args[0]  # Extract additional arguments\n",
    "        process_uncertainties(risks_list, labels_list, risks_list_by_class, labels_list_by_class, model_ft, dataloaders['test'], class_names, device, function, name, **additional_args)  # Pass additional arguments as keyword arguments\n",
    "    else:\n",
    "        process_uncertainties(risks_list, labels_list, risks_list_by_class, labels_list_by_class, model_ft, dataloaders['test'], class_names, device, function, name)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_list = [\n",
    "    \"Softmax Response measures the model's confidence in its predictions based on the softmax probabilities.\",\n",
    "    \"Top2 Softmax Difference measures the uncertainty by calculating the difference between the top two softmax probabilities\",\n",
    "    \"Random Uncertainties assigns random uncertainty values to each prediction, providing a baseline comparison for uncertainty estimation methods.\",\n",
    "    \"MCD By Sample utilizes Monte Carlo Dropout (MCD) to estimate uncertainties by averaging predictions across multiple samples with dropout.\",\n",
    "    \"MCD By Class employs Monte Carlo Dropout (MCD) to estimate uncertainties by averaging predictions across multiple samples with dropout for each class.\",\n",
    "    \"Variance calculates uncertainty by computing the variance of softmax probabilities across classes for each prediction.\",\n",
    "    \"Variational Ratio calculates uncertainty by computing the ratio of the maximum softmax probability (mode probability) to the maximum probability among other classes for each prediction.\",\n",
    "    \"Entropy calculates uncertainty by measuring the entropy of softmax probabilities for each prediction.\",\n",
    "    \"Predictive Entropy estimates uncertainty by averaging the entropy of softmax probabilities across multiple samples generated with dropout.\",\n",
    "    \"Mutual Information computes uncertainty by measuring the mutual information between the model's softmax probabilities and a uniform distribution.\"\n",
    "]\n",
    "\n",
    "for description in description_list:\n",
    "    print(f\"- {description}\")\n",
    "\n",
    "print(\n",
    "\"\"\"\n",
    "- Risk = 1 - Accuracy, where Accuracy is the mean accuracy (normalised) over the samples included in the Coverage.\n",
    "- Coverage is the normalised number of samples over the total samples. Coverage decreases as the most uncertain samples are removed.\n",
    "- Area Under Risk Coverage (AURC) is the area under the Risk Coverage curve.\n",
    "\n",
    "- Classes:\n",
    "    - Melanoma (MEL)\n",
    "    - Melanocytic nevus (NV)\n",
    "    - Basal cell carcinoma (BCC)\n",
    "    - Actinic keratosis (AK)\n",
    "    - Benign keratosis (BKL) \n",
    "        [solar lentigo / seborrheic keratosis / lichen planus-like keratosis]\n",
    "    - Dermatofibroma (DF)\n",
    "    - Vascular lesion (VASC)\n",
    "    - Squamous cell carcinoma (SCC)\n",
    "    - None of the others (UNK)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg6WwxuGrOsn"
   },
   "outputs": [],
   "source": [
    "# Define the desired metrics to plot\n",
    "# comment out undesired metrics\n",
    "desired_metrics = [\n",
    "    \"Softmax Response\",\n",
    "    \"Top2 Softmax Difference\",\n",
    "    \"Random Uncertainties\",\n",
    "    \"MCD By Sample\",\n",
    "    \"MCD By Class\",\n",
    "    \"Variance\",\n",
    "    \"Variational Ratio\",\n",
    "    \"Variational Ratio with Dropout\",\n",
    "    \"Entropy\",\n",
    "    \"Predictive Entropy\",\n",
    "    \"Mutual Information\",\n",
    "    \"Mutual Information with Dropout\"\n",
    "]\n",
    "\n",
    "# Select desired metrics for all samples\n",
    "selected_labels_list, selected_risks_list = select_desired_metrics(labels_list, risks_list, desired_metrics)\n",
    "\n",
    "# Print information about all classes\n",
    "print(f\"All Classes: {len(selected_risks_list[0])} samples\")\n",
    "x_smooth_percentage_interp, x_smooth_percentage = smooth_calcs(selected_risks_list[0])\n",
    "calculate_aurc(selected_risks_list, selected_labels_list, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "plot_risk_coverage(selected_risks_list, selected_labels_list, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "\n",
    "# Iterate over each class\n",
    "for i in range(len(class_names)):\n",
    "    # Select desired metrics for the current class\n",
    "    selected_labels_list_class, selected_risks_list_class = select_desired_metrics(labels_list_by_class[i], risks_list_by_class[i], desired_metrics)\n",
    "    \n",
    "    # Print information about the current class\n",
    "    print(f\"{class_names[i]}: {len(selected_risks_list_class[0])} samples\")\n",
    "    x_smooth_percentage_interp, x_smooth_percentage = smooth_calcs(selected_risks_list_class[0])\n",
    "    calculate_aurc(selected_risks_list_class, selected_labels_list_class, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "    plot_risk_coverage(selected_risks_list_class, selected_labels_list_class, x_smooth_percentage_interp, x_smooth_percentage)\n",
    "    print()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
